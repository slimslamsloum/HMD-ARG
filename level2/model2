"""
Module that provides a classifier to train a model on embeddings and predict
whether a protein is ARG or not. The dataset used is the antibiotic-resistance
from biodatasets, and the embedding of the 17k proteins come from the 
esm1_t34_670M_UR100 model.
"""

from string import digits
from typing import Counter

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from biodatasets import list_datasets, load_dataset
from deepchain.models import MLP
from deepchain.models.utils import (confusion_matrix_plot,
                                    model_evaluation_accuracy)
from joblib import dump, load
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from torch import Tensor, float32, float64, norm, randperm
from torchmetrics import F1, Accuracy, AveragePrecision, Precision, Recall

# Load embedding and target dataset
dataset = load_dataset("antibiotic-resistance")

X, y = dataset.to_npy_arrays(input_names=["sequence"], target_names=["label"])

cls_embeddings = np.load(
    "/home/selim/.cache/bio-datasets/antibiotic-resistance/sequence_esm1_t34_670M_UR100_cls_embeddings.npy",
    allow_pickle=True)

cls_embeddings = torch.tensor(np.vstack(cls_embeddings).astype(np.float))

_, atb_classes = dataset.to_npy_arrays(input_names=["sequence"], target_names=["antibiotic_class"])
atb_classes = atb_classes[0]
X = X[0][atb_classes == "beta_lactam"]

_, genes = dataset.to_npy_arrays(input_names=["sequence"], target_names=["gene_name"])
genes = np.array(genes[0]).astype(str)[atb_classes == "beta_lactam"]

genes = np.array(genes)
X = np.array(X)

label_assignment = {"TEM": 'A' , "SHV": 'A', "AmpC": 'C' , "OXA": 'D', "CcrA": 'B' , "IndB": 'B', "AmpC": 'C',
"CMY": 'C', "bla": 'A', "ACT" : 'C', "IMP": 'B', "VIM": 'B', "CARB": 'A', "cphA": 'B', "GES": 'A', "LEN": 'A',
"TLA": 'A', "OXY": 'A', "NDM": 'B', "MIR": 'C', "DHA": 'C', "KPC": 'A'}

samples = np.zeros((len(X),2)).astype(str)

for k in label_assignment:
    matching = [k in s for s in genes]
    subclass = np.full((len(X[matching])), label_assignment[k])
    samples[matching]= np.column_stack((X[matching], subclass))

mask = ['0' not in line[0] for line in samples]
samples = samples[mask]

#missing = []
#for gene in genes:
#    if not any(k in gene for k in label_assignment.keys()):
#        missing.append(gene)
#print(missing)
#print(len(np.unique(missing)))
#print(Counter(missing))

#remove_chars = str.maketrans('','',digits+'-'+'_')
#res = [s.translate(remove_chars) for s in missing]
#print(len(np.unique(res)))
#print(Counter(res))


#Separate data into training and test sets
useful_embedds = cls_embeddings[atb_classes == "beta_lactam"][mask]
x_train, x_test, y_train, y_test = train_test_split(useful_embedds, samples[:, 1], test_size=0.3)


#---------------------------------Logistic Regression-------------------------------------------------

#Create logreg model, compute predictions on test set and store model
#logreg = LogisticRegression(random_state=0, solver='liblinear').fit(x_train, y_train)
#y_pred = logreg.predict_proba(x_test)
#y_pred = np.array([x[1] for x in y_pred])



#-----------------------------------------------------------------------------------------------------

#---------------------------------SVM-------------------------------------------------

#Create svm model, compute predictions on test set and store model
#svm = SVC(probability=True).fit(x_train, y_train)
#y_pred = svm.predict_proba(x_test)
#y_pred = np.array([x[1] for x in y_pred])
#dump(svm, 'src/level0/svm0.joblib')


#-----------------------------------------------------------------------------------------------------

# Build a multi-layer-perceptron on top of embedding

# The fit method can handle all the arguments available in the
# 'trainer' class of pytorch lightening :
#               https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html
# Example arguments:
# * specifies all GPUs regardless of its availability :
#               Trainer(gpus=-1, auto_select_gpus=False, max_epochs=20)

#---------------------------------MLP-----------------------------------------------------------------

#n_class = len(np.unique(y_train))
#print(n_class)
#input_shape = x_train.shape[1]
#print(input_shape)

#mlp = MLP(input_shape=input_shape, n_class=n_class)
#mlp.fit(x_train, y_train, epochs=16)
#mlp.save("model.pt")

# Model evaluation
#y_pred = mlp(x_test).squeeze().detach().numpy()
#model_evaluation_accuracy(y_test, y_pred)

#------------------------------------------------------------------------------------------------------

# Plot confusion matrix
#confusion_matrix_plot(y_test, (y_pred > 0.5).astype(int), ["0", "1"])

# Accuracy evaluation
#accuracy = Accuracy()
#print('Accuracy: {0}'.format(accuracy(torch.from_numpy(y_pred), torch.from_numpy(y_test).int())))

# Precision evaluation
#precision = Precision()
#print('Precision: {0}'.format(precision(torch.from_numpy(y_pred), torch.from_numpy(y_test).int())))

# Recall evalution
#recall = Recall()
#print('Recall: {0}'.format(recall(torch.from_numpy(y_pred), torch.from_numpy(y_test).int())))

# F1 Score evaluation
#f1score = F1()
#print('F1 Score: {0}'.format(f1score(torch.from_numpy(y_pred), torch.from_numpy(y_test).int())))








